{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "157f4be2-9ef6-4ac4-8581-1cc446fe913d",
   "metadata": {},
   "source": [
    "# System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b6f4ab0-feed-4092-88ba-b49ef64a0499",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T15:30:11.162768Z",
     "iopub.status.busy": "2025-04-04T15:30:11.162623Z",
     "iopub.status.idle": "2025-04-04T15:30:11.164894Z",
     "shell.execute_reply": "2025-04-04T15:30:11.164566Z",
     "shell.execute_reply.started": "2025-04-04T15:30:11.162756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.uname()=posix.uname_result(sysname='Linux', nodename='rig', release='6.11.0-21-generic', version='#21~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Feb 24 16:52:15 UTC 2', machine='x86_64')\n",
      "os.cpu_count()=16\n",
      "os.get_exec_path()=['/home/mauricio/miniconda3/envs/dev/bin', '/home/mauricio/kafka/bin', '/home/mauricio/code/linux-scripts', '/home/mauricio/miniconda3/condabin', '/usr/local/sbin', '/usr/local/bin', '/usr/sbin', '/usr/bin', '/sbin', '/bin', '/usr/games', '/usr/local/games', '/snap/bin', '/snap/bin']\n",
      "python: 3.12.9 | packaged by conda-forge | (main, Mar  4 2025, 22:48:41) [GCC 13.3.0]\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "print(f'{os.uname()=}')\n",
    "print(f'{os.cpu_count()=}')\n",
    "print(f'{os.get_exec_path()=}')\n",
    "print('python:', sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cae2561-d4ca-4aa2-b693-3611ee26364b",
   "metadata": {},
   "source": [
    "# torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d523793-3228-4c2f-8091-d4f7a92aca97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T15:30:11.165330Z",
     "iopub.status.busy": "2025-04-04T15:30:11.165229Z",
     "iopub.status.idle": "2025-04-04T15:30:11.892819Z",
     "shell.execute_reply": "2025-04-04T15:30:11.892514Z",
     "shell.execute_reply.started": "2025-04-04T15:30:11.165319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__='2.4.1'\n",
      "torch.version.cuda='11.8'\n",
      "torch.version.debug=False\n",
      "torch.version.git_version='Unknown'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f'{torch.__version__=}')\n",
    "print(f'{torch.version.cuda=}')\n",
    "print(f'{torch.version.debug=}')\n",
    "print(f'{torch.version.git_version=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f1c671-2d28-4e67-b8ff-c3ebc8af67c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T18:27:48.060867Z",
     "iopub.status.busy": "2025-04-03T18:27:48.060743Z",
     "iopub.status.idle": "2025-04-03T18:27:48.063147Z",
     "shell.execute_reply": "2025-04-03T18:27:48.062936Z",
     "shell.execute_reply.started": "2025-04-03T18:27:48.060857Z"
    }
   },
   "source": [
    "# torch.backends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ddb493-938f-4bd5-915e-9b35e65555b3",
   "metadata": {},
   "source": [
    "## torch.backends.cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f71c3e42-ccea-48ad-86d1-f9b2923ee3fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T15:30:11.893347Z",
     "iopub.status.busy": "2025-04-04T15:30:11.893209Z",
     "iopub.status.idle": "2025-04-04T15:30:11.895903Z",
     "shell.execute_reply": "2025-04-04T15:30:11.895473Z",
     "shell.execute_reply.started": "2025-04-04T15:30:11.893334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.backends.cpu.get_cpu_capability()='AVX512'\n"
     ]
    }
   ],
   "source": [
    "print(f'{torch.backends.cpu.get_cpu_capability()=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a011e142-5d45-458b-b946-3f0d83121264",
   "metadata": {},
   "source": [
    "## torch.backends.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "064dec2c-10eb-4fcc-a333-18fa0126a2ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T15:30:11.896534Z",
     "iopub.status.busy": "2025-04-04T15:30:11.896373Z",
     "iopub.status.idle": "2025-04-04T15:30:11.930296Z",
     "shell.execute_reply": "2025-04-04T15:30:11.929908Z",
     "shell.execute_reply.started": "2025-04-04T15:30:11.896514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.backends.cuda.is_built()=True\n",
      "torch.backends.cuda.matmul.allow_tf32=False (controls True/False)\n",
      "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction=True (controls True/False)\n",
      "torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction=True (controls True/False)\n",
      "torch.backends.cuda.cufft_plan_cache[0]=<torch.backends.cuda.cuFFTPlanCache object at 0x7abb16f5e990>\n",
      "torch.backends.cuda.cufft_plan_cache.size=0\n",
      "torch.backends.cuda.cufft_plan_cache.max_size=4096 (controls Int)\n",
      "torch.backends.cuda.preferred_blas_library(backend=None)=<_BlasBackend.Cublas: 0> (controls \"cublas\"/\"cublaslt\")\n",
      "torch.backends.cuda.preferred_linalg_library(backend=None)=<_LinalgBackend.Default: 0> (controls \"cusolver\", \"magma\", \"default\")\n",
      "torch.backends.cuda.flash_sdp_enabled()=True\n",
      "torch.backends.cuda.enable_mem_efficient_sdp(True)=None (controls True/False)\n",
      "torch.backends.cuda.mem_efficient_sdp_enabled()=True\n",
      "torch.backends.cuda.enable_flash_sdp(True)=None (controls True/False)\n",
      "torch.backends.cuda.math_sdp_enabled()=True\n",
      "torch.backends.cuda.enable_math_sdp(True)=None (controls True/False)\n",
      "torch.backends.cuda.cudnn_sdp_enabled()=False\n",
      "torch.backends.cuda.enable_cudnn_sdp(True)=None\n",
      "torch.nn.attention.sdpa_kernel(backends=None)=<contextlib._GeneratorContextManager object at 0x7abc94657260>\n"
     ]
    }
   ],
   "source": [
    "print(f'{torch.backends.cuda.is_built()=}')\n",
    "print(f'{torch.backends.cuda.matmul.allow_tf32=} (controls True/False)')\n",
    "print(f'{torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction=} (controls True/False)')\n",
    "print(f'{torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction=} (controls True/False)')\n",
    "print(f'{torch.backends.cuda.cufft_plan_cache[0]=}')\n",
    "print(f'{torch.backends.cuda.cufft_plan_cache.size=}')\n",
    "print(f'{torch.backends.cuda.cufft_plan_cache.max_size=} (controls Int)')\n",
    "print(f'{torch.backends.cuda.preferred_blas_library(backend=None)=} (controls \"cublas\"/\"cublaslt\")')\n",
    "print(f'{torch.backends.cuda.preferred_linalg_library(backend=None)=} (controls \"cusolver\", \"magma\", \"default\")')\n",
    "# scale dot product\n",
    "print(f'{torch.backends.cuda.flash_sdp_enabled()=}')\n",
    "print(f'{torch.backends.cuda.enable_mem_efficient_sdp(True)=} (controls True/False)')\n",
    "\n",
    "print(f'{torch.backends.cuda.mem_efficient_sdp_enabled()=}')\n",
    "print(f'{torch.backends.cuda.enable_flash_sdp(True)=} (controls True/False)')\n",
    "\n",
    "print(f'{torch.backends.cuda.math_sdp_enabled()=}')\n",
    "print(f'{torch.backends.cuda.enable_math_sdp(True)=} (controls True/False)')\n",
    "# print(f'{torch.backends.cuda.fp16_bf16_reduction_math_sdp_allowed()=}') # requires newer pytorch?\n",
    "# print(f'{torch.backends.cuda.allow_fp16_bf16_reduction_math_sdp(True)=}') # requires newer pytorch?\n",
    "print(f'{torch.backends.cuda.cudnn_sdp_enabled()=}')\n",
    "print(f'{torch.backends.cuda.enable_cudnn_sdp(True)=}')\n",
    "# print(f'{torch.backends.cuda.is_flash_attention_available()=}') # requires newer pytorch?\n",
    "# print(f'{torch.backends.cuda.can_use_flash_attention(params, debug=False)=}') # requires newer pytorch?\n",
    "# print(f'{torch.backends.cuda.can_use_efficient_attention(params, debug=False)=}') # need params\n",
    "# print(f'{torch.backends.cuda.can_use_cudnn_attention(params, debug=False)=}') # requires newer pytorch?\n",
    "\n",
    "# print(f'{torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=True, enable_mem_efficient=True, enable_cudnn=True)=}')\n",
    "# deprecated, replaced by\n",
    "print(f'{torch.nn.attention.sdpa_kernel(backends=None)=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62b0035-0113-4aa6-8123-cb6876555c67",
   "metadata": {},
   "source": [
    "## torch.backends.cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a71a58d-3772-4132-a2ec-2fe43d8f6867",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T15:30:11.930754Z",
     "iopub.status.busy": "2025-04-04T15:30:11.930651Z",
     "iopub.status.idle": "2025-04-04T15:30:11.933656Z",
     "shell.execute_reply": "2025-04-04T15:30:11.933355Z",
     "shell.execute_reply.started": "2025-04-04T15:30:11.930742Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.backends.cudnn.version()=90800\n",
      "torch.backends.cudnn.is_available()=True\n",
      "torch.backends.cudnn.enabled=True (switchable True/False)\n",
      "torch.backends.cudnn.allow_tf32=True (switchable True/False)\n",
      "torch.backends.cudnn.deterministic=False (switchable True/False)\n",
      "torch.backends.cudnn.benchmark=False (switchable True/False)\n",
      "torch.backends.cudnn.benchmark_limit=10 (controls int)\n"
     ]
    }
   ],
   "source": [
    "print(f'{torch.backends.cudnn.version()=}')\n",
    "print(f'{torch.backends.cudnn.is_available()=}')\n",
    "print(f'{torch.backends.cudnn.enabled=} (switchable True/False)')\n",
    "print(f'{torch.backends.cudnn.allow_tf32=} (switchable True/False)')\n",
    "print(f'{torch.backends.cudnn.deterministic=} (switchable True/False)')\n",
    "print(f'{torch.backends.cudnn.benchmark=} (switchable True/False)')\n",
    "print(f'{torch.backends.cudnn.benchmark_limit=} (controls int)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400bccfb-4dc5-41f2-bcb0-19884dfe8eee",
   "metadata": {},
   "source": [
    "## torch.backends.cusparselt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e528c7e0-e327-456f-8485-ec1193bb87c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T15:30:11.935198Z",
     "iopub.status.busy": "2025-04-04T15:30:11.934942Z",
     "iopub.status.idle": "2025-04-04T15:30:11.936503Z",
     "shell.execute_reply": "2025-04-04T15:30:11.936294Z",
     "shell.execute_reply.started": "2025-04-04T15:30:11.935184Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(f'{torch.backends.cusparselt.version()=}') # requires newer pytorch?\n",
    "# print(f'{torch.backends.cusparselt.is_available()=}')  # requires newer pytorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f29570c-4121-43ac-929d-63df28873f68",
   "metadata": {},
   "source": [
    "## torch.backends.mha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcf6d679-4537-45f8-babd-08ec4621b56e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T15:30:11.936881Z",
     "iopub.status.busy": "2025-04-04T15:30:11.936792Z",
     "iopub.status.idle": "2025-04-04T15:30:11.938961Z",
     "shell.execute_reply": "2025-04-04T15:30:11.938672Z",
     "shell.execute_reply.started": "2025-04-04T15:30:11.936870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.backends.mha.get_fastpath_enabled()=True\n",
      "torch.backends.mha.set_fastpath_enabled(True)=None\n"
     ]
    }
   ],
   "source": [
    "print(f'{torch.backends.mha.get_fastpath_enabled()=}')\n",
    "print(f'{torch.backends.mha.set_fastpath_enabled(True)=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083b8082-214f-49c3-8e97-9f7315002016",
   "metadata": {},
   "source": [
    "## torch.backends.mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e9c7e71-4fdd-42a3-9621-e5bd993fc186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T15:30:11.939545Z",
     "iopub.status.busy": "2025-04-04T15:30:11.939393Z",
     "iopub.status.idle": "2025-04-04T15:30:11.941625Z",
     "shell.execute_reply": "2025-04-04T15:30:11.941347Z",
     "shell.execute_reply.started": "2025-04-04T15:30:11.939526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.backends.mps.is_available()=False\n",
      "torch.backends.mps.is_built()=False\n"
     ]
    }
   ],
   "source": [
    "print(f'{torch.backends.mps.is_available()=}')\n",
    "print(f'{torch.backends.mps.is_built()=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f2aa4b-7d39-43af-87ff-0f32f01703e8",
   "metadata": {},
   "source": [
    "## torch.backends.mkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b771dad-688a-4bb9-aaf4-3a426dd4b394",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T15:30:11.942289Z",
     "iopub.status.busy": "2025-04-04T15:30:11.942071Z",
     "iopub.status.idle": "2025-04-04T15:30:11.944564Z",
     "shell.execute_reply": "2025-04-04T15:30:11.944107Z",
     "shell.execute_reply.started": "2025-04-04T15:30:11.942269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.backends.mkl.is_available()=True\n",
      "torch.backends.mkl.verbose(torch.backends.mkl.VERBOSE_ON)=<torch.backends.mkl.verbose object at 0x7abb175fe4e0> (controls VERBOSE_ON, VERBOSE_OFF\n"
     ]
    }
   ],
   "source": [
    "print(f'{torch.backends.mkl.is_available()=}')\n",
    "print(f'{torch.backends.mkl.verbose(torch.backends.mkl.VERBOSE_ON)=} (controls VERBOSE_ON, VERBOSE_OFF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99efbd83-ee45-410e-a03c-45d2b653413e",
   "metadata": {},
   "source": [
    "## torch.backends.mkldnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8cf195e-b1d5-43ec-8b0c-b998c99f3a30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T15:30:11.945235Z",
     "iopub.status.busy": "2025-04-04T15:30:11.945054Z",
     "iopub.status.idle": "2025-04-04T15:30:11.947432Z",
     "shell.execute_reply": "2025-04-04T15:30:11.947145Z",
     "shell.execute_reply.started": "2025-04-04T15:30:11.945216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.backends.mkldnn.is_available()=False\n",
      "torch.backends.mkldnn.verbose(torch.backends.mkldnn.VERBOSE_ON)=<torch.backends.mkldnn.verbose object at 0x7abb16fcbad0> (controls VERBOSE_ON, VERBOSE_OFF)\n"
     ]
    }
   ],
   "source": [
    "print(f'{torch.backends.mkldnn.is_available()=}')\n",
    "print(f'{torch.backends.mkldnn.verbose(torch.backends.mkldnn.VERBOSE_ON)=} (controls VERBOSE_ON, VERBOSE_OFF)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebbecfa-30d5-4ddc-8a47-fd4a675f5dee",
   "metadata": {},
   "source": [
    "## torch.backends.nnpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "583c136f-f5bf-492d-814c-992dfb5f922a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T15:30:11.948197Z",
     "iopub.status.busy": "2025-04-04T15:30:11.947933Z",
     "iopub.status.idle": "2025-04-04T15:30:11.950391Z",
     "shell.execute_reply": "2025-04-04T15:30:11.950121Z",
     "shell.execute_reply.started": "2025-04-04T15:30:11.948177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.backends.nnpack.is_available()=True\n",
      "torch.backends.nnpack.flags(enabled=False)=<contextlib._GeneratorContextManager object at 0x7abb16fcbe60>\n",
      "torch.backends.nnpack.set_flags(True)=(True,)\n"
     ]
    }
   ],
   "source": [
    "print(f'{torch.backends.nnpack.is_available()=}')\n",
    "print(f'{torch.backends.nnpack.flags(enabled=False)=}')\n",
    "print(f'{torch.backends.nnpack.set_flags(True)=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f5bb15-1c0c-4514-9cf9-8321cb1d84d0",
   "metadata": {},
   "source": [
    "## torch.backends.openmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cc9695c-783b-4281-95a9-659417874dd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T15:30:11.951071Z",
     "iopub.status.busy": "2025-04-04T15:30:11.950846Z",
     "iopub.status.idle": "2025-04-04T15:30:11.953061Z",
     "shell.execute_reply": "2025-04-04T15:30:11.952771Z",
     "shell.execute_reply.started": "2025-04-04T15:30:11.951051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.backends.openmp.is_available()=True\n"
     ]
    }
   ],
   "source": [
    "print(f'{torch.backends.openmp.is_available()=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8866b10f-2cfb-428d-8daf-e7d7aa73f03c",
   "metadata": {},
   "source": [
    "## torch.backends.opt_einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bbce546-f480-4b66-8768-bf4a8a805816",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T15:30:11.953781Z",
     "iopub.status.busy": "2025-04-04T15:30:11.953548Z",
     "iopub.status.idle": "2025-04-04T15:30:11.955452Z",
     "shell.execute_reply": "2025-04-04T15:30:11.955146Z",
     "shell.execute_reply.started": "2025-04-04T15:30:11.953762Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(f'{torch.backends.opt_einsum.is_available()=}') # requires newer pytorch?\n",
    "# print(f'{torch.backends.opt_einsum.get_opt_einsum()=}') # requires newer pytorch?\n",
    "# print(f'{torch.backends.opt_einsum.enabled=}') # requires newer pytorch?\n",
    "# print(f'{torch.backends.opt_einsum.strategy=}') # requires newer pytorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca364a1-c883-402d-8aed-716196f1ef8c",
   "metadata": {},
   "source": [
    "## torch.backends.xeon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14cdef10-ef1e-459f-a6cc-ba5fc03d5812",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T15:30:11.956166Z",
     "iopub.status.busy": "2025-04-04T15:30:11.955903Z",
     "iopub.status.idle": "2025-04-04T15:30:11.957749Z",
     "shell.execute_reply": "2025-04-04T15:30:11.957471Z",
     "shell.execute_reply.started": "2025-04-04T15:30:11.956148Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(f'{torch.backends.xeon.is_available()=}') # requires newer pytorch?\n",
    "# print(f'{torch.backends.xeon.get_opt_einsum()=}') # requires newer pytorch?\n",
    "# print(f'{torch.backends.xeon.enabled=}') # requires newer pytorch?\n",
    "# print(f'{torch.backends.xeon.strategy=}') # requires newer pytorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b499cb81-cb37-4dda-97df-91637ac32b05",
   "metadata": {},
   "source": [
    "# torch.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b9af61a-5bf2-43d4-a838-bc9872ec1ff0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T15:43:27.102421Z",
     "iopub.status.busy": "2025-04-04T15:43:27.101915Z",
     "iopub.status.idle": "2025-04-04T15:43:27.126087Z",
     "shell.execute_reply": "2025-04-04T15:43:27.125854Z",
     "shell.execute_reply.started": "2025-04-04T15:43:27.102398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available()=True\n",
      "torch.cuda.device_count()=1\n",
      "torch.cuda.current_device()=0\n",
      "torch.cuda.get_device_name(torch.cuda.current_device())='NVIDIA GeForce RTX 3060'\n",
      "torch.cuda.get_device_capability()=(8, 6)\n",
      "torch.cuda.get_device_properties(torch.cuda.current_device())=_CudaDeviceProperties(name='NVIDIA GeForce RTX 3060', major=8, minor=6, total_memory=12004MB, multi_processor_count=28)\n",
      "device=device(type='cuda', index=0)\n",
      "torch.cuda.can_device_access_peer(0, 0)=False\n",
      "torch.cuda.current_blas_handle()=109915796826944\n",
      "torch.cuda.current_device()=0\n",
      "torch.cuda.current_stream()=<torch.cuda.Stream device=cuda:0 cuda_stream=0x0>\n",
      "torch.cuda.cudart()=<module 'torch._C._cudart'>\n",
      "torch.cuda.default_stream()=<torch.cuda.Stream device=cuda:0 cuda_stream=0x0>\n",
      "torch.cuda.device(0)=<torch.cuda.device object at 0x7abb16fca780>\n",
      "torch.cuda.device_count()=1\n",
      "torch.cuda.device_of(torch.tensor(0, device=\"cuda\"))=<torch.cuda.device_of object at 0x7abb17269fa0>\n",
      "torch.cuda.get_arch_list()=['sm_35', 'sm_50', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'sm_89', 'compute_89']\n",
      "torch.cuda.get_device_capability(0)=(8, 6)\n",
      "torch.cuda.get_device_name(0)='NVIDIA GeForce RTX 3060'\n",
      "torch.cuda.get_device_properties(0)=_CudaDeviceProperties(name='NVIDIA GeForce RTX 3060', major=8, minor=6, total_memory=12004MB, multi_processor_count=28)\n",
      "torch.cuda.get_gencode_flags()='-gencode compute=compute_35,code=sm_35 -gencode compute=compute_50,code=sm_50 -gencode compute=compute_60,code=sm_60 -gencode compute=compute_61,code=sm_61 -gencode compute=compute_70,code=sm_70 -gencode compute=compute_75,code=sm_75 -gencode compute=compute_80,code=sm_80 -gencode compute=compute_86,code=sm_86 -gencode compute=compute_89,code=sm_89 -gencode compute=compute_89,code=compute_89'\n",
      "torch.cuda.get_sync_debug_mode()=0\n",
      "torch.cuda.is_available()=True\n",
      "torch.cuda.is_initialized()=True\n",
      "torch.cuda.memory_usage()=65\n",
      "torch.cuda.set_device(0)=None\n",
      "torch.cuda.utilization()=0\n",
      "torch.cuda.temperature()=36\n",
      "torch.cuda.power_draw()=15055\n",
      "torch.cuda.clock_rate()=1522\n",
      "torch.cuda.OutOfMemoryError()=OutOfMemoryError()\n",
      "torch.cuda.get_rng_state()=tensor([ 15, 240, 102, 145,  51,  80,   8,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0], dtype=torch.uint8)\n",
      "torch.cuda.get_rng_state_all()=[tensor([ 15, 240, 102, 145,  51,  80,   8,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0], dtype=torch.uint8)]\n",
      "torch.cuda.set_rng_state(torch.ByteTensor([0, 0, 0, 0, 0, 0, 0,  0,  0,  0,  0,  0,  0,  0, 0,  0]))=None\n",
      "torch.cuda.set_rng_state_all([torch.ByteTensor([0, 0, 0, 0, 0, 0, 0,  0,  0,  0,  0,  0,  0,  0, 0,  0])])=None\n",
      "torch.cuda.manual_seed(0)=None\n",
      "torch.cuda.manual_seed_all(0)=None\n",
      "torch.cuda.seed()=None\n",
      "torch.cuda.seed_all()=None\n",
      "torch.cuda.initial_seed()=670584527669233\n",
      "torch.cuda.is_current_stream_capturing()=False\n",
      "torch.cuda.list_gpu_processes()='GPU:0\\nprocess       6930 uses      472.000 MB GPU memory\\nprocess       7659 uses      136.000 MB GPU memory'\n",
      "torch.cuda.mem_get_info()=(11161174016, 12587106304)\n",
      "torch.cuda.memory_stats()=OrderedDict({'active.all.allocated': 3, 'active.all.current': 1, 'active.all.freed': 2, 'active.all.peak': 2, 'active.large_pool.allocated': 1, 'active.large_pool.current': 1, 'active.large_pool.freed': 0, 'active.large_pool.peak': 1, 'active.small_pool.allocated': 2, 'active.small_pool.current': 0, 'active.small_pool.freed': 2, 'active.small_pool.peak': 1, 'active_bytes.all.allocated': 8520704, 'active_bytes.all.current': 8519680, 'active_bytes.all.freed': 1024, 'active_bytes.all.peak': 8520192, 'active_bytes.large_pool.allocated': 8519680, 'active_bytes.large_pool.current': 8519680, 'active_bytes.large_pool.freed': 0, 'active_bytes.large_pool.peak': 8519680, 'active_bytes.small_pool.allocated': 1024, 'active_bytes.small_pool.current': 0, 'active_bytes.small_pool.freed': 1024, 'active_bytes.small_pool.peak': 512, 'allocated_bytes.all.allocated': 8520704, 'allocated_bytes.all.current': 8519680, 'allocated_bytes.all.freed': 1024, 'allocated_bytes.all.peak': 8520192, 'allocated_bytes.large_pool.allocated': 8519680, 'allocated_bytes.large_pool.current': 8519680, 'allocated_bytes.large_pool.freed': 0, 'allocated_bytes.large_pool.peak': 8519680, 'allocated_bytes.small_pool.allocated': 1024, 'allocated_bytes.small_pool.current': 0, 'allocated_bytes.small_pool.freed': 1024, 'allocated_bytes.small_pool.peak': 512, 'allocation.all.allocated': 3, 'allocation.all.current': 1, 'allocation.all.freed': 2, 'allocation.all.peak': 2, 'allocation.large_pool.allocated': 1, 'allocation.large_pool.current': 1, 'allocation.large_pool.freed': 0, 'allocation.large_pool.peak': 1, 'allocation.small_pool.allocated': 2, 'allocation.small_pool.current': 0, 'allocation.small_pool.freed': 2, 'allocation.small_pool.peak': 1, 'inactive_split.all.allocated': 3, 'inactive_split.all.current': 1, 'inactive_split.all.freed': 2, 'inactive_split.all.peak': 2, 'inactive_split.large_pool.allocated': 1, 'inactive_split.large_pool.current': 1, 'inactive_split.large_pool.freed': 0, 'inactive_split.large_pool.peak': 1, 'inactive_split.small_pool.allocated': 2, 'inactive_split.small_pool.current': 0, 'inactive_split.small_pool.freed': 2, 'inactive_split.small_pool.peak': 1, 'inactive_split_bytes.all.allocated': 16645120, 'inactive_split_bytes.all.current': 12451840, 'inactive_split_bytes.all.freed': 4193280, 'inactive_split_bytes.all.peak': 14548480, 'inactive_split_bytes.large_pool.allocated': 12451840, 'inactive_split_bytes.large_pool.current': 12451840, 'inactive_split_bytes.large_pool.freed': 0, 'inactive_split_bytes.large_pool.peak': 12451840, 'inactive_split_bytes.small_pool.allocated': 4193280, 'inactive_split_bytes.small_pool.current': 0, 'inactive_split_bytes.small_pool.freed': 4193280, 'inactive_split_bytes.small_pool.peak': 2096640, 'max_split_size': -1, 'num_alloc_retries': 0, 'num_device_alloc': 2, 'num_device_free': 0, 'num_ooms': 0, 'num_sync_all_streams': 0, 'oversize_allocations.allocated': 0, 'oversize_allocations.current': 0, 'oversize_allocations.freed': 0, 'oversize_allocations.peak': 0, 'oversize_segments.allocated': 0, 'oversize_segments.current': 0, 'oversize_segments.freed': 0, 'oversize_segments.peak': 0, 'requested_bytes.all.allocated': 8519696, 'requested_bytes.all.current': 8519680, 'requested_bytes.all.freed': 16, 'requested_bytes.all.peak': 8519688, 'requested_bytes.large_pool.allocated': 8519680, 'requested_bytes.large_pool.current': 8519680, 'requested_bytes.large_pool.freed': 0, 'requested_bytes.large_pool.peak': 8519680, 'requested_bytes.small_pool.allocated': 16, 'requested_bytes.small_pool.current': 0, 'requested_bytes.small_pool.freed': 16, 'requested_bytes.small_pool.peak': 8, 'reserved_bytes.all.allocated': 23068672, 'reserved_bytes.all.current': 23068672, 'reserved_bytes.all.freed': 0, 'reserved_bytes.all.peak': 23068672, 'reserved_bytes.large_pool.allocated': 20971520, 'reserved_bytes.large_pool.current': 20971520, 'reserved_bytes.large_pool.freed': 0, 'reserved_bytes.large_pool.peak': 20971520, 'reserved_bytes.small_pool.allocated': 2097152, 'reserved_bytes.small_pool.current': 2097152, 'reserved_bytes.small_pool.freed': 0, 'reserved_bytes.small_pool.peak': 2097152, 'segment.all.allocated': 2, 'segment.all.current': 2, 'segment.all.freed': 0, 'segment.all.peak': 2, 'segment.large_pool.allocated': 1, 'segment.large_pool.current': 1, 'segment.large_pool.freed': 0, 'segment.large_pool.peak': 1, 'segment.small_pool.allocated': 1, 'segment.small_pool.current': 1, 'segment.small_pool.freed': 0, 'segment.small_pool.peak': 1})\n",
      "torch.cuda.memory_summary()\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   8320 KiB |   8320 KiB |   8321 KiB |   1024 B   |\n",
      "|       from large pool |   8320 KiB |   8320 KiB |   8320 KiB |      0 B   |\n",
      "|       from small pool |      0 KiB |      0 KiB |      1 KiB |   1024 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   8320 KiB |   8320 KiB |   8321 KiB |   1024 B   |\n",
      "|       from large pool |   8320 KiB |   8320 KiB |   8320 KiB |      0 B   |\n",
      "|       from small pool |      0 KiB |      0 KiB |      1 KiB |   1024 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   8320 KiB |   8320 KiB |   8320 KiB |     16 B   |\n",
      "|       from large pool |   8320 KiB |   8320 KiB |   8320 KiB |      0 B   |\n",
      "|       from small pool |      0 KiB |      0 KiB |      0 KiB |     16 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |\n",
      "|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |\n",
      "|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  12160 KiB |  14207 KiB |  16255 KiB |   4095 KiB |\n",
      "|       from large pool |  12160 KiB |  12160 KiB |  12160 KiB |      0 KiB |\n",
      "|       from small pool |      0 KiB |   2047 KiB |   4095 KiB |   4095 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       1    |       2    |       3    |       2    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       0    |       1    |       2    |       2    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       1    |       2    |       3    |       2    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       0    |       1    |       2    |       2    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       2    |       2    |       2    |       0    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       1    |       2    |       3    |       2    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       0    |       1    |       2    |       2    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "torch.cuda.memory_snapshot()=[{'device': 0, 'address': 134943141265408, 'total_size': 20971520, 'allocated_size': 8519680, 'active_size': 8519680, 'requested_size': 8519680, 'stream': 0, 'segment_type': 'large', 'segment_pool_id': (0, 0), 'is_expandable': False, 'frames': [], 'blocks': [{'address': 134943141265408, 'size': 8519680, 'requested_size': 8519680, 'state': 'active_allocated', 'frames': []}, {'address': 134943149785088, 'size': 12451840, 'requested_size': 0, 'state': 'inactive', 'frames': []}]}, {'device': 0, 'address': 134943162236928, 'total_size': 2097152, 'allocated_size': 0, 'active_size': 0, 'requested_size': 0, 'stream': 0, 'segment_type': 'small', 'segment_pool_id': (0, 0), 'is_expandable': False, 'frames': [], 'blocks': [{'address': 134943162236928, 'size': 2097152, 'requested_size': 8, 'state': 'inactive', 'frames': []}]}]\n",
      "torch.cuda.memory_allocated()=8519680\n",
      "torch.cuda.memory_reserved()=23068672\n",
      "torch.cuda.max_memory_reserved()=23068672\n",
      "torch.cuda.get_allocator_backend()='native'\n"
     ]
    }
   ],
   "source": [
    "print(f'{torch.cuda.is_available()=}')\n",
    "print(f'{torch.cuda.device_count()=}')\n",
    "print(f'{torch.cuda.current_device()=}')\n",
    "print(f'{torch.cuda.get_device_name(torch.cuda.current_device())=}')\n",
    "print(f'{torch.cuda.get_device_capability()=}')\n",
    "print(f'{torch.cuda.get_device_properties(torch.cuda.current_device())=}')\n",
    "device = torch.device(torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "print(f'{device=}')\n",
    "print(f'{torch.cuda.can_device_access_peer(0, 0)=}')\n",
    "print(f'{torch.cuda.current_blas_handle()=}')\n",
    "print(f'{torch.cuda.current_device()=}')\n",
    "print(f'{torch.cuda.current_stream()=}')\n",
    "print(f'{torch.cuda.cudart()=}')\n",
    "print(f'{torch.cuda.default_stream()=}')\n",
    "print(f'{torch.cuda.device(0)=}')\n",
    "print(f'{torch.cuda.device_count()=}')\n",
    "# print(f'{torch.cuda.device_memory_used=}') # requires newer pytorch?\n",
    "print(f'{torch.cuda.device_of(torch.tensor(0, device=\"cuda\"))=}')\n",
    "print(f'{torch.cuda.get_arch_list()=}')\n",
    "print(f'{torch.cuda.get_device_capability(0)=}')\n",
    "print(f'{torch.cuda.get_device_name(0)=}')\n",
    "print(f'{torch.cuda.get_device_properties(0)=}')\n",
    "print(f'{torch.cuda.get_gencode_flags()=}')\n",
    "print(f'{torch.cuda.get_sync_debug_mode()=}')\n",
    "print(f'{torch.cuda.is_available()=}')\n",
    "print(f'{torch.cuda.is_initialized()=}')\n",
    "print(f'{torch.cuda.memory_usage()=}')\n",
    "print(f'{torch.cuda.set_device(0)=}')  \n",
    "print(f'{torch.cuda.utilization()=}')\n",
    "print(f'{torch.cuda.temperature()=}')\n",
    "print(f'{torch.cuda.power_draw()=}') \n",
    "print(f'{torch.cuda.clock_rate()=}')\n",
    "print(f'{torch.cuda.OutOfMemoryError()=}')\n",
    "# Random Number Generator\n",
    "print(f'{torch.cuda.get_rng_state()=}')\n",
    "print(f'{torch.cuda.get_rng_state_all()=}')\n",
    "print(f'{torch.cuda.set_rng_state(torch.ByteTensor([0, 0, 0, 0, 0, 0, 0,  0,  0,  0,  0,  0,  0,  0, 0,  0]))=}')\n",
    "print(f'{torch.cuda.set_rng_state_all([torch.ByteTensor([0, 0, 0, 0, 0, 0, 0,  0,  0,  0,  0,  0,  0,  0, 0,  0])])=}')\n",
    "print(f'{torch.cuda.manual_seed(0)=}')\n",
    "print(f'{torch.cuda.manual_seed_all(0)=}')\n",
    "print(f'{torch.cuda.seed()=}')\n",
    "print(f'{torch.cuda.seed_all()=}')\n",
    "print(f'{torch.cuda.initial_seed()=}')\n",
    "print(f'{torch.cuda.is_current_stream_capturing()=}')\n",
    "# print(f'{torch.cuda.get_per_process_memory_fraction()=}') # requires newer pytorch?\n",
    "print(f'{torch.cuda.list_gpu_processes()=}') # pynvml module not found, please install pynvml\n",
    "print(f'{torch.cuda.mem_get_info()=}')\n",
    "print(f'{torch.cuda.memory_stats()=}')\n",
    "print(f'torch.cuda.memory_summary()\\n{torch.cuda.memory_summary()}')\n",
    "print(f'{torch.cuda.memory_snapshot()=}')\n",
    "print(f'{torch.cuda.memory_allocated()=}')\n",
    "print(f'{torch.cuda.memory_reserved()=}')\n",
    "print(f'{torch.cuda.max_memory_reserved()=}')\n",
    "print(f'{torch.cuda.get_allocator_backend()=}')\n",
    "# print(f'{torch.cuda.caching_allocator_enable=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eaff3deb-9cc3-490e-b2f3-f268f07bf947",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T15:30:12.034934Z",
     "iopub.status.busy": "2025-04-04T15:30:12.034830Z",
     "iopub.status.idle": "2025-04-04T15:30:12.037815Z",
     "shell.execute_reply": "2025-04-04T15:30:12.037519Z",
     "shell.execute_reply.started": "2025-04-04T15:30:12.034922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc --version\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Mar_28_02:18:24_PDT_2024\n",
      "Cuda compilation tools, release 12.4, V12.4.131\n",
      "Build cuda_12.4.r12.4/compiler.34097967_0\n"
     ]
    }
   ],
   "source": [
    "from subprocess import call\n",
    "print('nvcc', \"--version\")\n",
    "_ = call([\"nvcc\", \"--version\"]) # does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f75103bc-3937-4a20-822e-230435056672",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T15:30:12.038276Z",
     "iopub.status.busy": "2025-04-04T15:30:12.038169Z",
     "iopub.status.idle": "2025-04-04T15:30:12.050813Z",
     "shell.execute_reply": "2025-04-04T15:30:12.050525Z",
     "shell.execute_reply.started": "2025-04-04T15:30:12.038263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvidia-smi devices:\n",
      "index, name, driver_version, memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "0, NVIDIA GeForce RTX 3060, 550.120, 12288 MiB, 1305 MiB, 10700 MiB\n"
     ]
    }
   ],
   "source": [
    "print('nvidia-smi devices:')\n",
    "_ = call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
