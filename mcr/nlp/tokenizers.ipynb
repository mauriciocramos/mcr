{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f312add-c870-4222-8457-eee765767e47",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ec0867a-3517-4e2b-9c63-f9098285b89b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T10:08:04.110888Z",
     "iopub.status.busy": "2022-10-23T10:08:04.110700Z",
     "iopub.status.idle": "2022-10-23T10:08:04.642547Z",
     "shell.execute_reply": "2022-10-23T10:08:04.642142Z",
     "shell.execute_reply.started": "2022-10-23T10:08:04.110855Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth=1000\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.style.use('dark_background')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, NLTKWordTokenizer, wordpunct_tokenize\n",
    "from nltk.tokenize.regexp import RegexpTokenizer, WordPunctTokenizer, WhitespaceTokenizer, BlanklineTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '/home/mauricio/code/mcr')\n",
    "from mcr.nlp import token_count, CUSTOM_WORD_TOKENIZER_REGEX, SKLEARN_WORD_TOKENIZER_REGEX\n",
    "\n",
    "from mcr.nlp import WordTokenizer, SentenceTokenizer\n",
    "from mcr.nlp import sentence_count, word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70c76e1a-a430-4532-a778-4504732688db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T10:08:04.643057Z",
     "iopub.status.busy": "2022-10-23T10:08:04.642910Z",
     "iopub.status.idle": "2022-10-23T10:08:04.645872Z",
     "shell.execute_reply": "2022-10-23T10:08:04.645626Z",
     "shell.execute_reply.started": "2022-10-23T10:08:04.643046Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Punkt knows that the periods in Mr. Smith and Johann S. Bach\n",
      "do not mark sentence boundaries.  And sometimes sentences\n",
      "can start with non-capitalized words.  i is a good variable\n",
      "name.\n",
      "\n",
      "(How does it deal with this parenthesis?)  \"It should be part of the\n",
      "previous sentence.\" \"(And the same with this one.)\" ('And this one!')\n",
      "\"('(And (this)) '?)\" [(and this. )]\n",
      "Good muffins cost $3.88\n",
      "in New York.  Please buy me\n",
      "two of them.\n",
      "\n",
      "Thanks.\n"
     ]
    }
   ],
   "source": [
    "text = ''\n",
    "text += '''\n",
    "Punkt knows that the periods in Mr. Smith and Johann S. Bach\n",
    "do not mark sentence boundaries.  And sometimes sentences\n",
    "can start with non-capitalized words.  i is a good variable\n",
    "name.\n",
    "'''\n",
    "text += '''\n",
    "(How does it deal with this parenthesis?)  \"It should be part of the\n",
    "previous sentence.\" \"(And the same with this one.)\" ('And this one!')\n",
    "\"('(And (this)) '?)\" [(and this. )]\n",
    "'''\n",
    "text += 'Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.'\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95011af-6677-4f70-9ac6-d77c2051de53",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca65fb23-71e3-4121-afa4-59479cab696a",
   "metadata": {},
   "source": [
    "## Sentences\n",
    "\n",
    "Based on nltk.sent_tokenize (pre-trained PunktSentenceTokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef475481-0cf5-43d4-b879-7e4ce1ffee49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T10:08:04.646436Z",
     "iopub.status.busy": "2022-10-23T10:08:04.646301Z",
     "iopub.status.idle": "2022-10-23T10:08:04.656907Z",
     "shell.execute_reply": "2022-10-23T10:08:04.656650Z",
     "shell.execute_reply.started": "2022-10-23T10:08:04.646426Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nPunkt knows that the periods in Mr. Smith and Johann S. Bach\\ndo not mark sentence boundaries.',\n",
       " 'And sometimes sentences\\ncan start with non-capitalized words.',\n",
       " 'i is a good variable\\nname.',\n",
       " '(How does it deal with this parenthesis?)',\n",
       " '\"It should be part of the\\nprevious sentence.\"',\n",
       " '\"(And the same with this one.)\"',\n",
       " \"('And this one!')\",\n",
       " '\"(\\'(And (this)) \\'?)\"',\n",
       " '[(and this. )]',\n",
       " 'Good muffins cost $3.88\\nin New York.',\n",
       " 'Please buy me\\ntwo of them.',\n",
       " 'Thanks.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(SentenceTokenizer().tokenize(text))\n",
    "(SentenceTokenizer().tokenize(text) == SentenceTokenizer('english').tokenize(text)) &\\\n",
    "(SentenceTokenizer().tokenize(text) == SentenceTokenizer(language='english').tokenize(text)) &\\\n",
    "(SentenceTokenizer().tokenize(text) == sent_tokenize(text)) &\\\n",
    "(SentenceTokenizer().tokenize(text) == sent_tokenize(text, 'english')) &\\\n",
    "(SentenceTokenizer().tokenize(text) == sent_tokenize(text, language='english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74170889-5d3e-442d-a924-e97132903197",
   "metadata": {},
   "source": [
    "## Blank line paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "600af04f-ae8d-4124-9cab-d8098ed24c02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T10:08:04.657827Z",
     "iopub.status.busy": "2022-10-23T10:08:04.657692Z",
     "iopub.status.idle": "2022-10-23T10:08:04.661124Z",
     "shell.execute_reply": "2022-10-23T10:08:04.660861Z",
     "shell.execute_reply.started": "2022-10-23T10:08:04.657817Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nPunkt knows that the periods in Mr. Smith and Johann S. Bach\\ndo not mark sentence boundaries.  And sometimes sentences\\ncan start with non-capitalized words.  i is a good variable\\nname.',\n",
       " '(How does it deal with this parenthesis?)  \"It should be part of the\\nprevious sentence.\" \"(And the same with this one.)\" (\\'And this one!\\')\\n\"(\\'(And (this)) \\'?)\" [(and this. )]\\nGood muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.',\n",
       " 'Thanks.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# disabling sentence in WordTokenizer(preserve_line=True) and using BlankTokenizer() to separate paragraá¹•hs\n",
    "display(WordTokenizer(tokenizer=BlanklineTokenizer(), preserve_line=True).tokenize(text))\n",
    "(WordTokenizer(tokenizer=BlanklineTokenizer(), preserve_line=True).tokenize(text) == BlanklineTokenizer().tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004065d7-763a-4523-8ed0-eda1ad1de37b",
   "metadata": {},
   "source": [
    "## Words from Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1807529-0185-4549-9b1d-9c438c4ed01b",
   "metadata": {},
   "source": [
    "### Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e27e5a5c-7a18-4a32-9595-1e32a5e9eeaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T10:08:04.661558Z",
     "iopub.status.busy": "2022-10-23T10:08:04.661461Z",
     "iopub.status.idle": "2022-10-23T10:08:04.666502Z",
     "shell.execute_reply": "2022-10-23T10:08:04.666235Z",
     "shell.execute_reply.started": "2022-10-23T10:08:04.661548Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Punkt', 'knows', 'that', 'the', 'periods', 'in', 'Mr', 'Smith', 'and', 'Johann', 'S', 'Bach', 'do', 'not', 'mark', 'sentence', 'boundaries', 'And', 'sometimes', 'sentences', 'can', 'start', 'with', 'non', 'capitalized', 'words', 'i', 'is', 'a', 'good', 'variable', 'name', 'How', 'does', 'it', 'deal', 'with', 'this', 'parenthesis', 'It', 'should', 'be', 'part', 'of', 'the', 'previous', 'sentence', 'And', 'the', 'same', 'with', 'this', 'one', 'And', 'this', 'one', 'And', 'this', 'and', 'this', 'Good', 'muffins', 'cost', '3', '88', 'in', 'New', 'York', 'Please', 'buy', 'me', 'two', 'of', 'them', 'Thanks']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordTokenizer().tokenize(text)\n",
    "# same as\n",
    "WordTokenizer(pattern=CUSTOM_WORD_TOKENIZER_REGEX).tokenize(text)\n",
    "# same as\n",
    "print(WordTokenizer(tokenizer=RegexpTokenizer(CUSTOM_WORD_TOKENIZER_REGEX)).tokenize(text))\n",
    "\n",
    "(WordTokenizer().tokenize(text) == WordTokenizer(pattern=CUSTOM_WORD_TOKENIZER_REGEX).tokenize(text)) &\\\n",
    "(WordTokenizer().tokenize(text) == WordTokenizer(tokenizer=RegexpTokenizer(CUSTOM_WORD_TOKENIZER_REGEX)).tokenize(text)) &\\\n",
    "(WordTokenizer().tokenize(text) == WordTokenizer(tokenizer=RegexpTokenizer(CUSTOM_WORD_TOKENIZER_REGEX, gaps=False)).tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f44f5d-c5df-4a14-8c2e-a7efad3027cc",
   "metadata": {},
   "source": [
    "### NLTKWordTokenizer/TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d89c649c-9613-432d-8ab6-fe0730f05e81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T10:08:04.667016Z",
     "iopub.status.busy": "2022-10-23T10:08:04.666886Z",
     "iopub.status.idle": "2022-10-23T10:08:04.671328Z",
     "shell.execute_reply": "2022-10-23T10:08:04.671058Z",
     "shell.execute_reply.started": "2022-10-23T10:08:04.667006Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Punkt', 'knows', 'that', 'the', 'periods', 'in', 'Mr.', 'Smith', 'and', 'Johann', 'S.', 'Bach', 'do', 'not', 'mark', 'sentence', 'boundaries', '.', 'And', 'sometimes', 'sentences', 'can', 'start', 'with', 'non-capitalized', 'words', '.', 'i', 'is', 'a', 'good', 'variable', 'name', '.', '(', 'How', 'does', 'it', 'deal', 'with', 'this', 'parenthesis', '?', ')', '``', 'It', 'should', 'be', 'part', 'of', 'the', 'previous', 'sentence', '.', \"''\", '``', '(', 'And', 'the', 'same', 'with', 'this', 'one', '.', ')', \"''\", '(', \"'And\", 'this', 'one', '!', \"'\", ')', '``', '(', \"'\", '(', 'And', '(', 'this', ')', ')', \"'\", '?', ')', \"''\", '[', '(', 'and', 'this', '.', ')', ']', 'Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(WordTokenizer(tokenizer=NLTKWordTokenizer()).tokenize(text))\n",
    "word_tokenize(text) == WordTokenizer(tokenizer=NLTKWordTokenizer()).tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50f4e4a-9ee7-49ac-9fb0-5e9e4f6d4c8a",
   "metadata": {},
   "source": [
    "## Words without Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c84ed78-ebbf-4eac-8fb3-9028782b7069",
   "metadata": {},
   "source": [
    "### Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4bc4b08-7cd6-47a1-8bb9-fad28528de73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T10:08:04.671861Z",
     "iopub.status.busy": "2022-10-23T10:08:04.671682Z",
     "iopub.status.idle": "2022-10-23T10:08:04.675314Z",
     "shell.execute_reply": "2022-10-23T10:08:04.675079Z",
     "shell.execute_reply.started": "2022-10-23T10:08:04.671851Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Punkt', 'knows', 'that', 'the', 'periods', 'in', 'Mr', 'Smith', 'and', 'Johann', 'S', 'Bach', 'do', 'not', 'mark', 'sentence', 'boundaries', 'And', 'sometimes', 'sentences', 'can', 'start', 'with', 'non', 'capitalized', 'words', 'i', 'is', 'a', 'good', 'variable', 'name', 'How', 'does', 'it', 'deal', 'with', 'this', 'parenthesis', 'It', 'should', 'be', 'part', 'of', 'the', 'previous', 'sentence', 'And', 'the', 'same', 'with', 'this', 'one', 'And', 'this', 'one', 'And', 'this', 'and', 'this', 'Good', 'muffins', 'cost', '3', '88', 'in', 'New', 'York', 'Please', 'buy', 'me', 'two', 'of', 'them', 'Thanks']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordTokenizer(preserve_line=True).tokenize(text)\n",
    "# same as\n",
    "WordTokenizer(pattern=CUSTOM_WORD_TOKENIZER_REGEX, preserve_line=True).tokenize(text)\n",
    "# same as\n",
    "print(WordTokenizer(tokenizer=RegexpTokenizer(CUSTOM_WORD_TOKENIZER_REGEX), preserve_line=True).tokenize(text))\n",
    "\n",
    "(WordTokenizer(preserve_line=True).tokenize(text) == WordTokenizer(pattern=CUSTOM_WORD_TOKENIZER_REGEX, preserve_line=True).tokenize(text)) &\\\n",
    "(WordTokenizer(preserve_line=True).tokenize(text) == WordTokenizer(tokenizer=RegexpTokenizer(CUSTOM_WORD_TOKENIZER_REGEX), preserve_line=True).tokenize(text)) &\\\n",
    "(WordTokenizer(preserve_line=True).tokenize(text) == WordTokenizer(tokenizer=RegexpTokenizer(CUSTOM_WORD_TOKENIZER_REGEX, gaps=False), preserve_line=True).tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1523fb78-0dd8-4bd6-819e-db0bd1412a20",
   "metadata": {},
   "source": [
    "### NLTKWordTokenizer/TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44fe46bc-85c9-4f40-8983-284e08081f7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T10:08:04.675828Z",
     "iopub.status.busy": "2022-10-23T10:08:04.675652Z",
     "iopub.status.idle": "2022-10-23T10:08:04.678624Z",
     "shell.execute_reply": "2022-10-23T10:08:04.678366Z",
     "shell.execute_reply.started": "2022-10-23T10:08:04.675818Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Punkt', 'knows', 'that', 'the', 'periods', 'in', 'Mr.', 'Smith', 'and', 'Johann', 'S.', 'Bach', 'do', 'not', 'mark', 'sentence', 'boundaries.', 'And', 'sometimes', 'sentences', 'can', 'start', 'with', 'non-capitalized', 'words.', 'i', 'is', 'a', 'good', 'variable', 'name.', '(', 'How', 'does', 'it', 'deal', 'with', 'this', 'parenthesis', '?', ')', '``', 'It', 'should', 'be', 'part', 'of', 'the', 'previous', 'sentence.', \"''\", '``', '(', 'And', 'the', 'same', 'with', 'this', 'one.', ')', \"''\", '(', \"'And\", 'this', 'one', '!', \"'\", ')', \"''\", '(', \"'\", '(', 'And', '(', 'this', ')', ')', \"'\", '?', ')', \"''\", '[', '(', 'and', 'this.', ')', ']', 'Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(WordTokenizer(tokenizer=NLTKWordTokenizer(), preserve_line=True).tokenize(text))\n",
    "NLTKWordTokenizer().tokenize(text) == WordTokenizer(tokenizer=NLTKWordTokenizer(), preserve_line=True).tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e79cf9d-2024-4045-b872-4b962ae0517d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tokenization count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30127127-e541-4980-9fae-f40a7008cf04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T10:08:04.679042Z",
     "iopub.status.busy": "2022-10-23T10:08:04.678953Z",
     "iopub.status.idle": "2022-10-23T10:08:04.681853Z",
     "shell.execute_reply": "2022-10-23T10:08:04.681562Z",
     "shell.execute_reply.started": "2022-10-23T10:08:04.679034Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 6, 'this': 5, 'the': 3, 'with': 3, 'good': 2, 'in': 2, 'it': 2, 'of': 2, 'one': 2, 'sentence': 2, '3': 1, '88': 1, 'a': 1, 'bach': 1, 'be': 1, 'boundaries': 1, 'buy': 1, 'can': 1, 'capitalized': 1, 'cost': 1, 'deal': 1, 'do': 1, 'does': 1, 'how': 1, 'i': 1, 'is': 1, 'johann': 1, 'knows': 1, 'mark': 1, 'me': 1, 'mr': 1, 'muffins': 1, 'name': 1, 'new': 1, 'non': 1, 'not': 1, 'parenthesis': 1, 'part': 1, 'periods': 1, 'please': 1, 'previous': 1, 'punkt': 1, 's': 1, 'same': 1, 'sentences': 1, 'should': 1, 'smith': 1, 'sometimes': 1, 'start': 1, 'thanks': 1, 'that': 1, 'them': 1, 'two': 1, 'variable': 1, 'words': 1, 'york': 1}\n"
     ]
    }
   ],
   "source": [
    "# Default custom sentence and word tokenizer\n",
    "print(token_count(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f53de61a-05bf-4ee4-89de-49c0c4d86b02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T10:08:04.682398Z",
     "iopub.status.busy": "2022-10-23T10:08:04.682210Z",
     "iopub.status.idle": "2022-10-23T10:08:04.684740Z",
     "shell.execute_reply": "2022-10-23T10:08:04.684357Z",
     "shell.execute_reply.started": "2022-10-23T10:08:04.682388Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 6, 'this': 5, 'the': 3, 'with': 3, 'good': 2, 'in': 2, 'it': 2, 'of': 2, 'one': 2, 'sentence': 2, '3': 1, '88': 1, 'a': 1, 'bach': 1, 'be': 1, 'boundaries': 1, 'buy': 1, 'can': 1, 'capitalized': 1, 'cost': 1, 'deal': 1, 'do': 1, 'does': 1, 'how': 1, 'i': 1, 'is': 1, 'johann': 1, 'knows': 1, 'mark': 1, 'me': 1, 'mr': 1, 'muffins': 1, 'name': 1, 'new': 1, 'non': 1, 'not': 1, 'parenthesis': 1, 'part': 1, 'periods': 1, 'please': 1, 'previous': 1, 'punkt': 1, 's': 1, 'same': 1, 'sentences': 1, 'should': 1, 'smith': 1, 'sometimes': 1, 'start': 1, 'thanks': 1, 'that': 1, 'them': 1, 'two': 1, 'variable': 1, 'words': 1, 'york': 1}\n"
     ]
    }
   ],
   "source": [
    "# No sentence, 1+ chararacters\n",
    "print(token_count(text, tokenizer=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60f290f8-f262-49cd-b31c-c7ecc1f2bf17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T10:08:04.685231Z",
     "iopub.status.busy": "2022-10-23T10:08:04.685120Z",
     "iopub.status.idle": "2022-10-23T10:08:04.687574Z",
     "shell.execute_reply": "2022-10-23T10:08:04.687334Z",
     "shell.execute_reply.started": "2022-10-23T10:08:04.685219Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 6, 'this': 5, 'the': 3, 'with': 3, 'good': 2, 'in': 2, 'it': 2, 'of': 2, 'one': 2, 'sentence': 2, '88': 1, 'bach': 1, 'be': 1, 'boundaries': 1, 'buy': 1, 'can': 1, 'capitalized': 1, 'cost': 1, 'deal': 1, 'do': 1, 'does': 1, 'how': 1, 'is': 1, 'johann': 1, 'knows': 1, 'mark': 1, 'me': 1, 'mr': 1, 'muffins': 1, 'name': 1, 'new': 1, 'non': 1, 'not': 1, 'parenthesis': 1, 'part': 1, 'periods': 1, 'please': 1, 'previous': 1, 'punkt': 1, 'same': 1, 'sentences': 1, 'should': 1, 'smith': 1, 'sometimes': 1, 'start': 1, 'thanks': 1, 'that': 1, 'them': 1, 'two': 1, 'variable': 1, 'words': 1, 'york': 1}\n"
     ]
    }
   ],
   "source": [
    "# No sentence, 2+ characters (as sklearn does)\n",
    "print(token_count(text, tokenizer=None, token_pattern=SKLEARN_WORD_TOKENIZER_REGEX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b18c6c1b-921b-47fe-8010-f5bdbf8d833e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T10:08:04.688088Z",
     "iopub.status.busy": "2022-10-23T10:08:04.687901Z",
     "iopub.status.idle": "2022-10-23T10:08:04.690224Z",
     "shell.execute_reply": "2022-10-23T10:08:04.689984Z",
     "shell.execute_reply.started": "2022-10-23T10:08:04.688078Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'(': 7, ')': 7, 'and': 5, \"''\": 4, 'this': 4, \"'\": 3, 'the': 3, 'with': 3, '?': 2, '``': 2, 'good': 2, 'in': 2, 'it': 2, 'of': 2, '!': 1, '$': 1, \"'and\": 1, '.': 1, '3.88': 1, '[': 1, ']': 1, 'a': 1, 'bach': 1, 'be': 1, 'boundaries.': 1, 'buy': 1, 'can': 1, 'cost': 1, 'deal': 1, 'do': 1, 'does': 1, 'how': 1, 'i': 1, 'is': 1, 'johann': 1, 'knows': 1, 'mark': 1, 'me': 1, 'mr.': 1, 'muffins': 1, 'name.': 1, 'new': 1, 'non-capitalized': 1, 'not': 1, 'one': 1, 'one.': 1, 'parenthesis': 1, 'part': 1, 'periods': 1, 'please': 1, 'previous': 1, 'punkt': 1, 's.': 1, 'same': 1, 'sentence': 1, 'sentence.': 1, 'sentences': 1, 'should': 1, 'smith': 1, 'sometimes': 1, 'start': 1, 'thanks': 1, 'that': 1, 'them.': 1, 'this.': 1, 'two': 1, 'variable': 1, 'words.': 1, 'york.': 1}\n"
     ]
    }
   ],
   "source": [
    "# No sentence, NLTKWordTokenizer\n",
    "print(token_count(text, tokenizer=NLTKWordTokenizer()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c2fe822-e06a-4c6c-9ed8-fd6a5669b3e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T10:08:04.691403Z",
     "iopub.status.busy": "2022-10-23T10:08:04.691211Z",
     "iopub.status.idle": "2022-10-23T10:08:04.693582Z",
     "shell.execute_reply": "2022-10-23T10:08:04.693307Z",
     "shell.execute_reply.started": "2022-10-23T10:08:04.691393Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 10, 'and': 6, 'this': 5, 'the': 3, 'with': 3, '(': 2, 'good': 2, 'in': 2, 'it': 2, 'of': 2, 'one': 2, 'sentence': 2, \"!')\": 1, '\"': 1, '\"(': 1, '\"(\\'(': 1, '$': 1, '\\'?)\"': 1, \"('\": 1, '))': 1, ')]': 1, '-': 1, '.\"': 1, '.)\"': 1, '3': 1, '88': 1, '?)': 1, '[(': 1, 'a': 1, 'bach': 1, 'be': 1, 'boundaries': 1, 'buy': 1, 'can': 1, 'capitalized': 1, 'cost': 1, 'deal': 1, 'do': 1, 'does': 1, 'how': 1, 'i': 1, 'is': 1, 'johann': 1, 'knows': 1, 'mark': 1, 'me': 1, 'mr': 1, 'muffins': 1, 'name': 1, 'new': 1, 'non': 1, 'not': 1, 'parenthesis': 1, 'part': 1, 'periods': 1, 'please': 1, 'previous': 1, 'punkt': 1, 's': 1, 'same': 1, 'sentences': 1, 'should': 1, 'smith': 1, 'sometimes': 1, 'start': 1, 'thanks': 1, 'that': 1, 'them': 1, 'two': 1, 'variable': 1, 'words': 1, 'york': 1}\n"
     ]
    }
   ],
   "source": [
    "# No sentence, WordPunctTokenizer\n",
    "print(token_count(text, tokenizer=WordPunctTokenizer()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6abe3a5-2280-4b32-bb12-503d6783317b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T10:08:04.694091Z",
     "iopub.status.busy": "2022-10-23T10:08:04.693981Z",
     "iopub.status.idle": "2022-10-23T10:08:04.696465Z",
     "shell.execute_reply": "2022-10-23T10:08:04.696150Z",
     "shell.execute_reply.started": "2022-10-23T10:08:04.694081Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 3, 'this': 3, 'with': 3, 'and': 2, 'good': 2, 'in': 2, 'of': 2, '\"(\\'(and': 1, '\"(and': 1, '\"it': 1, '$3.88': 1, '\\'?)\"': 1, \"('and\": 1, '(how': 1, '(this))': 1, ')]': 1, '[(and': 1, 'a': 1, 'bach': 1, 'be': 1, 'boundaries.': 1, 'buy': 1, 'can': 1, 'cost': 1, 'deal': 1, 'do': 1, 'does': 1, 'i': 1, 'is': 1, 'it': 1, 'johann': 1, 'knows': 1, 'mark': 1, 'me': 1, 'mr.': 1, 'muffins': 1, 'name.': 1, 'new': 1, 'non-capitalized': 1, 'not': 1, \"one!')\": 1, 'one.)\"': 1, 'parenthesis?)': 1, 'part': 1, 'periods': 1, 'please': 1, 'previous': 1, 'punkt': 1, 's.': 1, 'same': 1, 'sentence': 1, 'sentence.\"': 1, 'sentences': 1, 'should': 1, 'smith': 1, 'sometimes': 1, 'start': 1, 'thanks.': 1, 'that': 1, 'them.': 1, 'this.': 1, 'two': 1, 'variable': 1, 'words.': 1, 'york.': 1}\n"
     ]
    }
   ],
   "source": [
    "# No sentence, WhitespaceTokenizer\n",
    "print(token_count(text, tokenizer=WhitespaceTokenizer()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21d4a1e6-399f-4a2a-b267-eb9c97f965ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T10:08:04.697050Z",
     "iopub.status.busy": "2022-10-23T10:08:04.696831Z",
     "iopub.status.idle": "2022-10-23T10:08:04.699246Z",
     "shell.execute_reply": "2022-10-23T10:08:04.698989Z",
     "shell.execute_reply.started": "2022-10-23T10:08:04.697038Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\npunkt knows that the periods in mr. smith and johann s. bach\\ndo not mark sentence boundaries.  and sometimes sentences\\ncan start with non-capitalized words.  i is a good variable\\nname.': 1, '(how does it deal with this parenthesis?)  \"it should be part of the\\nprevious sentence.\" \"(and the same with this one.)\" (\\'and this one!\\')\\n\"(\\'(and (this)) \\'?)\" [(and this. )]\\ngood muffins cost $3.88\\nin new york.  please buy me\\ntwo of them.': 1, 'thanks.': 1}\n"
     ]
    }
   ],
   "source": [
    "# No sentence, BlanklineTokenizer\n",
    "print(token_count(text, tokenizer=BlanklineTokenizer()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d6391f7-caac-4707-bfff-ed43cf5fc3e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T10:08:04.699867Z",
     "iopub.status.busy": "2022-10-23T10:08:04.699613Z",
     "iopub.status.idle": "2022-10-23T10:08:04.706736Z",
     "shell.execute_reply": "2022-10-23T10:08:04.706440Z",
     "shell.execute_reply.started": "2022-10-23T10:08:04.699855Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\npunkt knows that the periods in mr. smith and johann s. bach\\ndo not mark sentence boundaries.': 1, '\"(\\'(and (this)) \\'?)\"': 1, '\"(and the same with this one.)\"': 1, '\"it should be part of the\\nprevious sentence.\"': 1, \"('and this one!')\": 1, '(how does it deal with this parenthesis?)': 1, '[(and this. )]': 1, 'and sometimes sentences\\ncan start with non-capitalized words.': 1, 'good muffins cost $3.88\\nin new york.': 1, 'i is a good variable\\nname.': 1, 'please buy me\\ntwo of them.': 1, 'thanks.': 1}\n"
     ]
    }
   ],
   "source": [
    "# Just sentences\n",
    "print(token_count(text, tokenizer=SentenceTokenizer(language='portuguese')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd9f497-ade5-4bb9-9452-69fe2f15f2f6",
   "metadata": {},
   "source": [
    "# Simple sentence and word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9ccf7f7-fed8-441a-8a3e-eb94ece89b45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T10:08:04.707245Z",
     "iopub.status.busy": "2022-10-23T10:08:04.707102Z",
     "iopub.status.idle": "2022-10-23T10:08:04.709650Z",
     "shell.execute_reply": "2022-10-23T10:08:04.709409Z",
     "shell.execute_reply.started": "2022-10-23T10:08:04.707234Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_count(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89c347b3-56f1-41cb-ab28-b312cb498c4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T10:08:04.710125Z",
     "iopub.status.busy": "2022-10-23T10:08:04.709997Z",
     "iopub.status.idle": "2022-10-23T10:08:04.712503Z",
     "shell.execute_reply": "2022-10-23T10:08:04.712242Z",
     "shell.execute_reply.started": "2022-10-23T10:08:04.710115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d46c2-1a5c-43fd-a1d8-fda2ed421c8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Corpus statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4bdd07e-5dd4-4e44-9420-f3e5cc99459d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T10:08:04.713127Z",
     "iopub.status.busy": "2022-10-23T10:08:04.712863Z",
     "iopub.status.idle": "2022-10-23T10:08:04.714740Z",
     "shell.execute_reply": "2022-10-23T10:08:04.714456Z",
     "shell.execute_reply.started": "2022-10-23T10:08:04.713116Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mcr.nlp import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "426d1b0a-8a37-4460-b3a7-850ca5e5cf6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T10:08:04.715210Z",
     "iopub.status.busy": "2022-10-23T10:08:04.715084Z",
     "iopub.status.idle": "2022-10-23T10:08:04.746490Z",
     "shell.execute_reply": "2022-10-23T10:08:04.746217Z",
     "shell.execute_reply.started": "2022-10-23T10:08:04.715199Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_bcc59\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_bcc59_level0_col0\" class=\"col_heading level0 col0\" >rows</th>\n",
       "      <th id=\"T_bcc59_level0_col1\" class=\"col_heading level0 col1\" >documents</th>\n",
       "      <th id=\"T_bcc59_level0_col2\" class=\"col_heading level0 col2\" >fill %</th>\n",
       "      <th id=\"T_bcc59_level0_col3\" class=\"col_heading level0 col3\" >unique documents</th>\n",
       "      <th id=\"T_bcc59_level0_col4\" class=\"col_heading level0 col4\" >unique %</th>\n",
       "      <th id=\"T_bcc59_level0_col5\" class=\"col_heading level0 col5\" >sentences</th>\n",
       "      <th id=\"T_bcc59_level0_col6\" class=\"col_heading level0 col6\" >sentences / document</th>\n",
       "      <th id=\"T_bcc59_level0_col7\" class=\"col_heading level0 col7\" >words</th>\n",
       "      <th id=\"T_bcc59_level0_col8\" class=\"col_heading level0 col8\" >words / document</th>\n",
       "      <th id=\"T_bcc59_level0_col9\" class=\"col_heading level0 col9\" >words / sentence</th>\n",
       "      <th id=\"T_bcc59_level0_col10\" class=\"col_heading level0 col10\" >unique words</th>\n",
       "      <th id=\"T_bcc59_level0_col11\" class=\"col_heading level0 col11\" >characters</th>\n",
       "      <th id=\"T_bcc59_level0_col12\" class=\"col_heading level0 col12\" >characters / document</th>\n",
       "      <th id=\"T_bcc59_level0_col13\" class=\"col_heading level0 col13\" >chars / sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_bcc59_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_bcc59_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_bcc59_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "      <td id=\"T_bcc59_row0_col2\" class=\"data row0 col2\" >100.00</td>\n",
       "      <td id=\"T_bcc59_row0_col3\" class=\"data row0 col3\" >1</td>\n",
       "      <td id=\"T_bcc59_row0_col4\" class=\"data row0 col4\" >100.00</td>\n",
       "      <td id=\"T_bcc59_row0_col5\" class=\"data row0 col5\" >12</td>\n",
       "      <td id=\"T_bcc59_row0_col6\" class=\"data row0 col6\" >12</td>\n",
       "      <td id=\"T_bcc59_row0_col7\" class=\"data row0 col7\" >75</td>\n",
       "      <td id=\"T_bcc59_row0_col8\" class=\"data row0 col8\" >75</td>\n",
       "      <td id=\"T_bcc59_row0_col9\" class=\"data row0 col9\" >6</td>\n",
       "      <td id=\"T_bcc59_row0_col10\" class=\"data row0 col10\" >56</td>\n",
       "      <td id=\"T_bcc59_row0_col11\" class=\"data row0 col11\" >435</td>\n",
       "      <td id=\"T_bcc59_row0_col12\" class=\"data row0 col12\" >435</td>\n",
       "      <td id=\"T_bcc59_row0_col13\" class=\"data row0 col13\" >36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f5e03ca8d60>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics(text).to_frame().T\\\n",
    "    .style.format('{:,.0f}').format('{:.2f}', subset=['fill %', 'unique %'])\\\n",
    "    #.background_gradient(axis=0, cmap='RdYlGn') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
